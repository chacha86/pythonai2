{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaIFfGczVvxDf2ZeZx9Wz0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chacha86/pythonai2/blob/main/%EB%84%A4%EC%9D%B4%EB%B2%84_%EB%9E%AD%ED%82%B9_%EB%89%B4%EC%8A%A4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "siEZ5KeszOMb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cdc6c25-b3c9-47bb-90f8-765c6e345dce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'SBS',\n",
              " 'link': 'https://media.naver.com/press/055/ranking?type=popular'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# 1. 언론사 이름과 링크 가져오기\n",
        "## 파이썬 웹 요청 라이브러리\n",
        "import requests\n",
        "\n",
        "\n",
        "## 파이썬이 브라우저인 척 하기 위함\n",
        "headers = {\n",
        "    'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'\n",
        "}\n",
        "\n",
        "## 네이버 랭킹 뉴스 웹페이지 요청. 성공하면 응답이옴(r)\n",
        "r = requests.get('https://news.naver.com/main/ranking/popularDay.naver', headers=headers)\n",
        "\n",
        "\n",
        "## 응답 정보에는 많은 게 있지만 우린 text가져가면 된다. text는 단순 문자열이다.\n",
        "ranking_news_html = r.text\n",
        "\n",
        "## 뷰티풀숩으로 파싱\n",
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(ranking_news_html, 'html.parser')\n",
        "\n",
        "# 하나만 선택 -> select_one\n",
        "# 여러개 선택 -> select\n",
        "\n",
        "box_list = soup.select(\"._officeCard .rankingnews_box\")\n",
        "\n",
        "\n",
        "journal_list = []\n",
        "\n",
        "for box in box_list :\n",
        "  atag = box.select_one('a')\n",
        "\n",
        "  # 태그를 얻었을 때\n",
        "  # 1. 텍스트\n",
        "  # 2. 속성\n",
        "  journal_link = atag['href']\n",
        "  journal_name = atag.select_one('strong').text\n",
        "\n",
        "  journal_dic = {\n",
        "      'name' : journal_name,\n",
        "      'link' : journal_link\n",
        "  }\n",
        "\n",
        "  journal_list.append(journal_dic)\n",
        "\n",
        "\n",
        "def get_journal_by_name(name) :\n",
        "\n",
        "  for journal in journal_list:\n",
        "    if journal['name'] == name :\n",
        "      return journal\n",
        "  return None\n",
        "\n",
        "get_journal_by_name(\"SBS\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. url을 넘기면 해당 url 페이지의 html을 파싱해서 soup를 만들어주는 함수\n",
        "def get_page_by_url(url) :\n",
        "  from bs4 import BeautifulSoup\n",
        "  import requests\n",
        "\n",
        "  headers = {\n",
        "    'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'\n",
        "  }\n",
        "  r = requests.get(url, headers=headers)\n",
        "  soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "  return soup\n"
      ],
      "metadata": {
        "id": "rrPhoif2zXy7"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. page를 넘겨주면 해당 page에서 랭킹 뉴스 20개를 가져오는 함수\n",
        "sbs = get_journal_by_name(\"SBS\")\n",
        "url = sbs['link']\n",
        "news_page = get_page_by_url(url)\n",
        "\n",
        "def get_rankingnews_by_page(page) :\n",
        "  thumb_list = page.select('.as_thumb')\n",
        "\n",
        "  news_list = []\n",
        "\n",
        "  for thumb in thumb_list :\n",
        "    link = thumb.select_one('a')['href'] # 본문링크\n",
        "    rank = thumb.select_one('em').text # 랭킹\n",
        "    title = thumb.select_one(\".list_content strong\").text  # 제목\n",
        "    tmp = thumb.select_one(\".list_content .list_view\").text #  조회수,\n",
        "    view = tmp[4:].strip()\n",
        "\n",
        "    news_dic = {\n",
        "        'link' : link,\n",
        "        'rank' : rank,\n",
        "        'title' : title,\n",
        "        'view' : view\n",
        "    }\n",
        "\n",
        "    news_list.append(news_dic)\n",
        "\n",
        "  return news_list\n",
        "\n",
        "sbs_ranking_news = get_rankingnews_by_page(news_page)"
      ],
      "metadata": {
        "id": "MJmIUnenzdsC"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# 4. 파이썬 오브젝트를 json으로 저장해주는 함수\n",
        "def save_to_json(filepath, obj) :\n",
        "  with open(filepath, 'w') as f :\n",
        "    json_str = json.dumps(obj, ensure_ascii=False)\n",
        "    f.write(json_str)\n"
      ],
      "metadata": {
        "id": "khcK-P4Wzk6_"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. json 파일을 파이썬 오브젝트로 불러오는 함수\n",
        "def load_json(filepath) :\n",
        "  with open(filepath, 'r') as f:\n",
        "    json_str = f.read()\n",
        "    return json.loads(json_str)"
      ],
      "metadata": {
        "id": "s7Gfl0aAzrQ4"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_to_json('sbs_news.json', sbs_ranking_news)"
      ],
      "metadata": {
        "id": "WHOmfdfbF7aw"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sbs_news = load_json('sbs_news.json')\n",
        "sbs_news"
      ],
      "metadata": {
        "id": "NhRxHpsZGGhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RXFYtW__zwsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 실습 -> 3개의 언론사 (SBS, MBC, KBS)의 랭킹 뉴스를 최근 5일치 가져와서 각각 저장.\n",
        "# 언론사와 n을 넘기면 해당 언론사의 랭킹뉴스를 n일치 가져오기\n",
        "# 언론사별로 폴더를 나눠서 저장\n",
        "\n",
        "def get_n_latest_news(journal, n) :\n",
        "\n",
        "\n",
        "\n",
        "get_n_latest_news('SBS', 5)"
      ],
      "metadata": {
        "id": "ppPnO4Xe0Dyr"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "EaU1WBgPGU8f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}